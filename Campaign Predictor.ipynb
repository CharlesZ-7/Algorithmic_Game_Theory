{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5f6feb-5c52-4671-95ed-c830ace15db0",
   "metadata": {},
   "source": [
    "We adopt dynamical (binary search) prices for impression vector as a probe, namely the price = budget cutoff for the untargeted marget segment.\n",
    "If we win the advertising auction, then we shade down the price; If we lose the advertising auction, we scale up the price.\n",
    "The price vector (8, ) at the end of the n-th day is fed into the LSTM neural network.\n",
    "The open campaigns are revealed to us. We maintain a container which keeps track of those open campaigns on the (n+1)-th day and can be represented as a campaign vector (26, ). This campaign vector is also fed into the LSTM neural network. The goal of the LSTM neural network is to predict the hidden campaigns on the (n+1)-th day. The output of the LSTM neural network is a vector of softmax function (26, ), representing the probability of each campaign. Then we can reconstruct the campaigns with reach = budget = probability * 10000 / 3.\n",
    "The LSTM neural network is pretrained but will also be trained online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d802f3a-7615-4e19-bdfb-04fb5d5bcbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import LBFGS, NAdam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6553256c-80ff-4665-97b8-69512dfbad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CampaignPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(CampaignPredictor, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # LSTM with specified number of layers and neurons\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Xavier normal initialization\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_normal_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.xavier_normal_(param.data)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return torch.sigmoid(out)  # Softmax is applied later in the training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efdb3553-f167-4a44-b0ae-25abd4b027df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CampaignDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sequence = self.sequences[index]\n",
    "        # Assuming each sequence is a tuple (features, target)\n",
    "        features = torch.tensor(sequence[0], dtype=torch.float32)\n",
    "        target = torch.tensor(sequence[1], dtype=torch.float32)\n",
    "        return features, target\n",
    "\n",
    "# train_loader = ...\n",
    "# test_loader = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670afb67-e81d-48b7-bbd7-6d5e216db3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 34\n",
    "output_dim = 26\n",
    "hidden_dim = 100\n",
    "layer_dim = 3\n",
    "NAdam_iter = 5000\n",
    "LBFGS_iter = 3000\n",
    "batch_size = 1000\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "NAdam_optimizer = NAdam(model.parameters(), lr=0.001)\n",
    "LBFGS_optimizer = LBFGS(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c50c05f2-436b-49f6-86c9-b0be64e56dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, epochs, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(features.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(features.float())\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "        \n",
    "            optimizer.step(closure)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93169a44-dd0f-4611-b150-2b97865d935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_credits = 0  # Total credits earned, including partial credits\n",
    "        total_possible = 0  # Total possible credits (each label element counts as one credit)\n",
    "\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features.float())\n",
    "            \n",
    "            # Multiply probabilities by 10 and round to nearest integer\n",
    "            predicted = (outputs * 10).round()\n",
    "\n",
    "            # Compute the absolute difference between predicted and true labels\n",
    "            difference = torch.abs(predicted - labels)\n",
    "\n",
    "            # Full credit for exact matches, half credit for off by one\n",
    "            credits = (difference == 0).float() + (difference == 1).float() * 0.5\n",
    "            \n",
    "            total_credits += credits.sum().item()\n",
    "            total_possible += labels.numel()  # Number of label elements processed\n",
    "\n",
    "        # Calculate the percentage of total credits earned out of total possible credits\n",
    "        accuracy_percentage = 100 * total_credits / total_possible\n",
    "        print(f'Accuracy (with partial credits): {accuracy_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa9794d2-4578-486f-a646-3491688e9214",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m CampaignPredictor(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m train(model, \u001b[43mtrain_loader\u001b[49m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      5\u001b[0m evaluate(model, test_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CampaignPredictor(input_size, hidden_size, num_layers, output_size)\n",
    "train(model, NAdam_optimizer, NAdam_iter, train_loader, epochs, device=device)\n",
    "train(model, LBFGS_optimizer, LBFGS_iter, train_loader, epochs, device=device)\n",
    "evaluate(model, test_loader, device=device)\n",
    "\n",
    "model(features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
